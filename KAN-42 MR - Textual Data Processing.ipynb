{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a4e5c1-58eb-4841-a1e4-b839cf39ba54",
   "metadata": {},
   "source": [
    "# KAN-42 Textual Data Processing\n",
    "by Miguel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749225d5-2cbe-4119-b037-d58dfe23c7a4",
   "metadata": {},
   "source": [
    "**Step 1: Install Required Libraries**\n",
    "\n",
    "\n",
    "First, ensure you have the necessary libraries installed. You'll need the datasets library for loading the dataset, pandas for data manipulation, and nltk for text processing. If you don't have these installed, you can install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18db23d-043a-4bc3-960c-372ac0c3be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (2.2.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from pandas) (2024.2)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\021348\\appdata\\local\\anaconda3\\envs\\medvqa_project\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets pandas nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b84b6-28c6-4f6a-842c-ac2ece5b78a7",
   "metadata": {},
   "source": [
    "**Step 2: Load the SLAKE Dataset**\n",
    "\n",
    "Load the SLAKE dataset using the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19cc0212-b754-4a38-978b-10feada0118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\021348\\AppData\\Local\\anaconda3\\envs\\medVQA_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  \n",
    "  \n",
    "# Load the SLAKE dataset  \n",
    "ds = load_dataset(\"BoKelvin/SLAKE\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480f879-a438-485f-b566-1183352abfea",
   "metadata": {},
   "source": [
    "**Step 3: Extract Question-Answer Pairs**\n",
    "\r\n",
    "Inspect the dataset to identify the structure and extract the question-answer pairs. Assuming the dataset contains columns for questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f6c670-c87b-489e-82cc-9de0df8b771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n",
      "        num_rows: 9835\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n",
      "        num_rows: 2099\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n",
      "        num_rows: 2094\n",
      "    })\n",
      "})\n",
      "{'img_name': 'xmlab1/source.jpg', 'location': 'Abdomen', 'answer': 'MRI', 'modality': 'MRI', 'base_type': 'vqa', 'answer_type': 'OPEN', 'question': 'What modality is used to take this image?', 'qid': 0, 'content_type': 'Modality', 'triple': ['vhead', '_', '_'], 'img_id': 1, 'q_lang': 'en'}\n",
      "                                            question   answer  \\\n",
      "0          What modality is used to take this image?      MRI   \n",
      "1  Which part of the body does this image belong to?  Abdomen   \n",
      "2            What is the mr weighting in this image?       T2   \n",
      "3                    Does the picture contain liver?      Yes   \n",
      "4                   Does the picture contain kidney?       No   \n",
      "\n",
      "            img_name  \n",
      "0  xmlab1/source.jpg  \n",
      "1  xmlab1/source.jpg  \n",
      "2  xmlab1/source.jpg  \n",
      "3  xmlab1/source.jpg  \n",
      "4  xmlab1/source.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Display dataset structure  \n",
    "print(ds)  \n",
    "  \n",
    "# Display the first example to understand the structure  \n",
    "print(ds['train'][0])  \n",
    "  \n",
    "# Convert to Pandas DataFrame for easier manipulation  \n",
    "import pandas as pd  \n",
    "  \n",
    "# Convert the train split to a DataFrame  \n",
    "df_train = pd.DataFrame(ds['train'])  \n",
    "  \n",
    "# Extract question-answer pairs  \n",
    "qa_pairs = df_train[['question', 'answer','img_name']]  \n",
    "print(qa_pairs.head())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2b9d8-3ddd-464e-a886-a5356a6e9caf",
   "metadata": {},
   "source": [
    "**Step 4: Clean and Preprocess Text Data**\n",
    "\n",
    "We'll perform the following preprocessing steps:\n",
    "* Lowercase the text\n",
    "* Remove punctiation\n",
    "* Tokenize the text\n",
    "* Remove stop words\n",
    "\n",
    "We'll use the re library for regular expressions and nltk for stop words and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f768df31-2982-4245-8f8e-2d67a41f592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\021348\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\021348\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\021348\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0          What modality is used to take this image?   \n",
      "1  Which part of the body does this image belong to?   \n",
      "2            What is the mr weighting in this image?   \n",
      "3                    Does the picture contain liver?   \n",
      "4                   Does the picture contain kidney?   \n",
      "\n",
      "                                 processed_question   answer processed_answer  \\\n",
      "0          what modality is used to take this image      MRI              mri   \n",
      "1  which part of the body does this image belong to  Abdomen          abdomen   \n",
      "2            what is the mr weighting in this image       T2               t2   \n",
      "3                    does the picture contain liver      Yes              yes   \n",
      "4                   does the picture contain kidney       No               no   \n",
      "\n",
      "            img_name  \n",
      "0  xmlab1/source.jpg  \n",
      "1  xmlab1/source.jpg  \n",
      "2  xmlab1/source.jpg  \n",
      "3  xmlab1/source.jpg  \n",
      "4  xmlab1/source.jpg  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\021348\\AppData\\Local\\Temp\\ipykernel_14992\\689904561.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_pairs['processed_question'] = qa_pairs['question'].apply(preprocess_text)\n",
      "C:\\Users\\021348\\AppData\\Local\\Temp\\ipykernel_14992\\689904561.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qa_pairs['processed_answer'] = qa_pairs['answer'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "  \n",
    "# Download NLTK data (you can skip this if already downloaded)  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "  \n",
    "# Function to preprocess text  \n",
    "def preprocess_text(text):  \n",
    "    # Lowercase the text  \n",
    "    text = text.lower()  \n",
    "      \n",
    "    # Remove punctuation  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "      \n",
    "    # Tokenize the text  \n",
    "    words = word_tokenize(text)  \n",
    "      \n",
    "    # Remove stop words  -- commented out as answers may be yes/no or loose context if stopwrods are removed\n",
    "    #stop_words = set(stopwords.words('english'))  \n",
    "    #words = [word for word in words if word not in stop_words]  \n",
    "      \n",
    "    return ' '.join(words)  \n",
    "  \n",
    "# Apply preprocessing to the question and answer columns  \n",
    "qa_pairs['processed_question'] = qa_pairs['question'].apply(preprocess_text)  \n",
    "qa_pairs['processed_answer'] = qa_pairs['answer'].apply(preprocess_text)  \n",
    "  \n",
    "# Display the first few rows of the processed text  \n",
    "print(qa_pairs[['question', 'processed_question', 'answer', 'processed_answer','img_name']].head())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e4d25-ab04-4383-b403-4a6fd98d16e6",
   "metadata": {},
   "source": [
    "**Step 5: Tokenize Text for Input into the PubMedCLIP Model**\n",
    "\r\n",
    "To tokenize text for input into the PubMedCLIP model, you would typically use a tokenizer that is compatible with the model.\n",
    " However, since PubMedCLIP is a specialized model, make sure to use the appropriate tokenizer.\n",
    "\n",
    " Below is an example of how you might do it with a generic tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dde1c1-5db3-4ba1-80cf-361e628341a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "  \n",
    "# Load the PubMedCLIP tokenizer (replace 'pubmedclip-tokenizer' with the actual tokenizer name)  \n",
    "tokenizer = AutoTokenizer.from_pretrained('pubmedclip-tokenizer')  \n",
    "  \n",
    "# Tokenize the processed questions and answers  \n",
    "qa_pairs['tokenized_question'] = qa_pairs['processed_question'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))  \n",
    "qa_pairs['tokenized_answer'] = qa_pairs['processed_answer'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))  \n",
    "  \n",
    "# Display the first few rows of the token\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
